{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qkX7DiM6rmeM"
   },
   "source": [
    "# Pose Estimation\n",
    "Compare to PyTorch3D `Camera position optimization` sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# io utils\n",
    "from pytorch3d.io import load_objs_as_meshes\n",
    "\n",
    "# 3D transformations functions\n",
    "from pytorch3d.transforms import so3_log_map, axis_angle_to_matrix, axis_angle_to_quaternion, quaternion_to_matrix\n",
    "\n",
    "# rendering components\n",
    "from pytorch3d.renderer import (\n",
    "    FoVPerspectiveCameras, look_at_view_transform, \n",
    "    RasterizationSettings, MeshRenderer, MeshRasterizer,\n",
    "    HardPhongShader, PointLights, TexturesVertex\n",
    ")\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and generate views with PyTorch3D\n",
    "we're using the cow model from Keenan Crane, featured in the PyTorch3D tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch_device = torch.device('cuda')\n",
    "else:\n",
    "    torch_device = torch.device('cpu')\n",
    "mesh = load_objs_as_meshes(['data/cow.obj'], device=torch_device)\n",
    "\n",
    "# seems sane to fetch/estimate scale\n",
    "shape_scale = float(mesh.verts_list()[0].std(0).mean())*3 \n",
    "t_model_scale = np.ptp(np.array(mesh.verts_list()[0].cpu()),0).mean()\n",
    "print('model is {:.2f}x the size of the cow'.format(shape_scale/1.18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is simply the dataset generation code, taken from the PyTorch3D tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (64,64)\n",
    "vfov_degrees = 60\n",
    "\n",
    "lights = PointLights(device=torch_device, location=[[0.0, 0.0, -3.0*shape_scale]])\n",
    "R, T = look_at_view_transform(dist=2.7*shape_scale, elev=60, azim=20,device=torch_device)\n",
    "camera = FoVPerspectiveCameras(device=torch_device,znear=shape_scale, zfar=100*shape_scale, fov=vfov_degrees) \n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=image_size, \n",
    "    blur_radius=0.0, \n",
    "    faces_per_pixel=1, \n",
    ")\n",
    "\n",
    "renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=camera, \n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=HardPhongShader(\n",
    "        device=torch_device, \n",
    "        cameras=camera,\n",
    "        lights=lights\n",
    "    )\n",
    ")\n",
    "depth_raster =MeshRasterizer(\n",
    "    cameras=camera, \n",
    "    raster_settings=raster_settings\n",
    ")\n",
    "\n",
    "# Render the cow mesh \n",
    "target_image = np.squeeze(renderer(mesh, camera=camera, lights=lights, R=R, T=T))\n",
    "target_depth = np.squeeze(depth_raster(mesh, camera=camera, lights=lights, R=R, T=T).zbuf)\n",
    "target_depth[target_depth == -1] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Fuzzy Metaball renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "\n",
    "import jax\n",
    "#jax.config.update('jax_platform_name', 'cpu')\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import fm_render\n",
    "\n",
    "# expected performance is roughly\n",
    "# saved (optimized shape!): 1.5, volume: 2.5, surface: 2.5\n",
    "\n",
    "show_saved = True\n",
    "show_volume = True\n",
    "if show_saved:\n",
    "    hyperparams = [2.00, -0.1,  6.4, -5.44]\n",
    "else:\n",
    "    if show_volume:\n",
    "        hyperparams = [2.58, 0.70, 6.64, -5.61]\n",
    "    else:\n",
    "        hyperparams = [1.84, 0.30, 5.44, -4.33]\n",
    "\n",
    "NUM_MIXTURE = 40\n",
    "beta2 = jnp.float32(np.exp(hyperparams[0]))\n",
    "beta3 = jnp.float32(np.exp(hyperparams[1]))\n",
    "beta4 = jnp.float32(np.exp(hyperparams[2]))\n",
    "beta5 = -jnp.float32(np.exp(hyperparams[3]))\n",
    "\n",
    "render_jit = jax.jit(fm_render.render_func_mrp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_saved:\n",
    "    import pickle\n",
    "    # load a model of the cow. feel free to use the one reconstructed from silhouettes!\n",
    "    with open('fuzzy_cow.pkl','rb') as fp:\n",
    "        mean,prec,weight_log = pickle.load(fp)\n",
    "    weights = np.exp(weight_log)\n",
    "    weights /= np.sum(weights)\n",
    "else:\n",
    "    import trimesh\n",
    "    import sklearn.mixture\n",
    "    tmesh = trimesh.Trimesh(mesh.cpu().verts_packed(),mesh.cpu().faces_packed())\n",
    "    if show_volume:\n",
    "        pts = trimesh.sample.volume_mesh(tmesh,10000)\n",
    "    else:\n",
    "        pts = trimesh.sample.sample_surface_even(tmesh,10000)[0]\n",
    "    gmm = sklearn.mixture.GaussianMixture(NUM_MIXTURE)\n",
    "    gmm.fit(pts)\n",
    "    weights = gmm.weights_\n",
    "    weight_log = np.log(weights)\n",
    "    mean = gmm.means_\n",
    "    prec = gmm.precisions_cholesky_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def axangle2mrp(axangle):\n",
    "    scale = jnp.linalg.norm(axangle)\n",
    "    vec = axangle/scale\n",
    "    return jnp.tan(scale/4)*vec\n",
    "\n",
    "def rot2mrp(axangle):\n",
    "    scale = jnp.linalg.norm(axangle)\n",
    "    vec = axangle/scale\n",
    "    return jnp.tan(scale/4)*vec\n",
    "    \n",
    "def convert_pyt3dcamera(cam, image_size = image_size):\n",
    "    height, width = image_size\n",
    "    cx = (width-1)/2\n",
    "    cy = (height-1)/2\n",
    "    f = (height/np.tan((np.pi/180)*float(cam.fov[0])/2))*0.5\n",
    "    K = np.array([[f, 0, cx],[0,f,cy],[0,0,1]])\n",
    "    pixel_list = (np.array(np.meshgrid(width-np.arange(width)-1,height-np.arange(height)-1,[0]))[:,:,:,0]).reshape((3,-1)).T\n",
    "\n",
    "    camera_rays = (pixel_list - K[:,2])/np.diag(K)\n",
    "    camera_rays[:,-1] = 1\n",
    "    return jnp.array(camera_rays), jnp.array(so3_log_map(cam.R)[0].cpu()), jnp.array(-cam.R[0].cpu()@cam.T[0].cpu())\n",
    "# get the real camera\n",
    "camera_rays, axangl_true, trans_true = convert_pyt3dcamera(camera)\n",
    "mrp_true = axangle2mrp(axangl_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add noise to pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    trans_eps = 0.5\n",
    "    rad_eps = (np.pi/180.0)*90 # range. so 90 is -45 to +45\n",
    "\n",
    "    trans_err = np.random.randn(3)\n",
    "    trans_err = trans_err/np.linalg.norm(trans_err)\n",
    "    trans_err_r = np.random.rand()\n",
    "\n",
    "    trans_err = trans_eps*trans_err*trans_err_r\n",
    "    trans_shift = np.array(trans_true) - trans_err\n",
    "\n",
    "    angles = np.random.randn(3)\n",
    "    angles = angles/np.linalg.norm(angles)\n",
    "    angle_mag = (np.random.rand()-0.5)*rad_eps\n",
    "    R_I = np.array(fm_render.mrp_to_rot(mrp_true).T)\n",
    "    R_R = np.array(axis_angle_to_matrix(torch.tensor(angles*angle_mag))).T\n",
    "    R_C =  R_R @ R_I\n",
    "\n",
    "    axangl_init = np.array(so3_log_map(torch.tensor(R_C[None])))[0]\n",
    "    trans_init = np.array(R_R@np.array(trans_shift))\n",
    "    mrp_init = axangle2mrp(axangl_init)\n",
    "\n",
    "    rand_rot = abs(angle_mag*(180.0/np.pi))\n",
    "    rand_trans = 100*(trans_err_r*trans_eps/t_model_scale)\n",
    "    init_pose_err = np.sqrt(rand_rot*rand_trans)\n",
    "    print('pose error of {:.1f}, random rotation of {:.1f} degrees and translation of {:.1f}%'.format(init_pose_err,rand_rot,rand_trans))\n",
    "    if rand_trans > 30 and rand_rot > 30:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2,2,1)\n",
    "plt.imshow(target_image.cpu())\n",
    "plt.title('image')\n",
    "plt.axis('off')\n",
    "plt.subplot(2,2,2)\n",
    "vmin,vmax = np.nanmin(target_depth.cpu()),np.nanmax(target_depth.cpu())\n",
    "plt.imshow(target_depth.cpu(),vmin=vmin,vmax=vmax)\n",
    "plt.title('depth')\n",
    "plt.axis('off')\n",
    "est_depth, est_probs, est_alpha = render_jit(mean,prec,weight_log,camera_rays,mrp_init,trans_init,beta2/shape_scale,beta3,beta4,beta5)\n",
    "est_depth = np.array(est_depth)\n",
    "est_depth[est_alpha < 0.5] = np.nan\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(est_alpha.reshape(image_size),cmap='Greys')\n",
    "plt.title('FM alpha')\n",
    "plt.axis('off')\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(est_depth.reshape(image_size),vmin=vmin,vmax=vmax)\n",
    "plt.title('FM depth')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve for camera pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_func(est_depth,est_alpha,true_depth):\n",
    "    cond = jnp.isnan(est_depth) | jnp.isnan(true_depth)\n",
    "    #err = (est_depth - true_depth)/jnp.nan_to_num(true_depth,nan=1)\n",
    "    err = (est_depth - true_depth)/jnp.nanmean(true_depth)\n",
    "\n",
    "    depth_loss =  (jnp.where(cond,0,err)**2).mean()\n",
    "\n",
    "    true_alpha = ~jnp.isnan(true_depth)\n",
    "    est_alpha = jnp.clip(est_alpha,1e-6,1-1e-6)\n",
    "    mask_loss = -((true_alpha * jnp.log(est_alpha)) + (~true_alpha)*jnp.log(1-est_alpha))\n",
    "\n",
    "    term1 = depth_loss.mean()\n",
    "    term2 = mask_loss.mean()\n",
    "    return 20*term1 + term2\n",
    "\n",
    "def objective(params,means,prec,weights_log,camera_rays,beta2,beta3,beta4,beta5,depth):\n",
    "    mrp,trans= params\n",
    "    render_res = render_jit(means,prec,weights_log,camera_rays,mrp,trans,beta2,beta3,beta4,beta5)\n",
    "    return error_func(render_res[0],render_res[2],depth)\n",
    "\n",
    "def objective_simple(params,means,prec,weights_log,camera_rays,beta2,beta3,beta4,beta5,depth):\n",
    "    mrp = jnp.array(params[:3])\n",
    "    trans = jnp.array(params[3:])\n",
    "    render_res = render_jit(means,prec,weights_log,camera_rays,mrp,trans,beta2,beta3,beta4,beta5)\n",
    "    return error_func(render_res[0],render_res[2],depth)\n",
    "grad_render3 = jax.jit(jax.value_and_grad(objective))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.example_libraries import optimizers\n",
    "from util import DegradeLR\n",
    "# Number of optimization steps\n",
    "# typically only needs a few hundred\n",
    "# and early exits\n",
    "Niter = 2000\n",
    "\n",
    "loop = tqdm(range(Niter))\n",
    "\n",
    "# babysit learning rates\n",
    "adjust_lr = DegradeLR(1e-3,0.1,50,10,-1e-4)\n",
    "opt_init, opt_update, opt_params = optimizers.momentum(adjust_lr.step_func,0.95)\n",
    "\n",
    "# to test scale invariance\n",
    "HUHSCALE = 1\n",
    "# should get same result even if world scale changes\n",
    "\n",
    "tmp = [mrp_init,HUHSCALE*trans_init]\n",
    "opt_state = opt_init(tmp)\n",
    "\n",
    "losses = []\n",
    "jax_tdepth = jnp.array(target_depth.cpu().ravel())\n",
    "\n",
    "for i in loop:\n",
    "    p = opt_params(opt_state)\n",
    "\n",
    "    val,g = grad_render3(p,HUHSCALE*mean,prec/HUHSCALE,weight_log,camera_rays,beta2/(HUHSCALE*shape_scale),beta3,beta4,beta5,HUHSCALE*jax_tdepth)\n",
    "    \n",
    "    S = jnp.linalg.norm(p[1])\n",
    "    S2 = S*S\n",
    "\n",
    "    g1 = g[0]\n",
    "    g2 = g[1]*S2\n",
    "\n",
    "    opt_state = opt_update(i, [g1,g2], opt_state)\n",
    "\n",
    "    val = float(val)\n",
    "    losses.append(val)\n",
    "    if adjust_lr.add(val):\n",
    "        break\n",
    "    # Print the losses\n",
    "    loop.set_description(\"total_loss = %.3f\" % val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrp_final, trans_final = opt_params(opt_state)\n",
    "trans_final = trans_final/HUHSCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd order is also possible\n",
    "if False:\n",
    "    from jax.scipy.optimize import minimize\n",
    "    res = minimize(objective_simple,jnp.hstack([mrp_init,trans_init]),method='BFGS',args=(mean,prec,weight_log,camera_rays,beta2,beta3,beta4,beta5,jax_tdepth,))\n",
    "    mrp_final = res.x[:3]\n",
    "    trans_final = res.x[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('convergence plot')\n",
    "plt.plot(losses,marker='.',lw=0,ms=5,alpha=0.5)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('log loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2,3,1)\n",
    "plt.imshow(target_image.cpu())\n",
    "plt.title('image')\n",
    "plt.axis('off')\n",
    "plt.subplot(2,3,4)\n",
    "est_depth_true, est_prob_true, est_alpha_true = render_jit(mean,prec,weight_log,camera_rays,mrp_true,trans_true,beta2/shape_scale,beta3,beta4,beta5)\n",
    "est_depth_true = np.array(est_depth_true)\n",
    "est_depth_true[est_alpha_true < 0.5] = np.nan\n",
    "plt.imshow(est_depth_true.reshape(image_size),vmin=vmin,vmax=vmax)\n",
    "plt.title('true pose')\n",
    "plt.axis('off')\n",
    "est_depth_init, est_probs,est_alpha = render_jit(mean,prec,weight_log,camera_rays,mrp_init,trans_init,beta2/shape_scale,beta3,beta4,beta5)\n",
    "est_depth_init = np.array(est_depth_init)\n",
    "est_depth_init[est_alpha < 0.5] = np.nan\n",
    "plt.subplot(2,3,2)\n",
    "plt.imshow(est_alpha.reshape(image_size),cmap='Greys')\n",
    "plt.title('init FM alpha')\n",
    "plt.axis('off')\n",
    "plt.subplot(2,3,5)\n",
    "plt.imshow(est_depth_init.reshape(image_size),vmin=vmin,vmax=vmax)\n",
    "plt.title('init FM depth')\n",
    "plt.axis('off')\n",
    "est_depth, est_probs, est_alpha = render_jit(mean,prec,weight_log,camera_rays,mrp_final,trans_final,beta2/shape_scale,beta3,beta4,beta5)\n",
    "est_depth = np.array(est_depth)\n",
    "est_depth[est_alpha < 0.5] = np.nan\n",
    "plt.subplot(2,3,3)\n",
    "plt.imshow(est_alpha.reshape(image_size),cmap='Greys')\n",
    "plt.title('final FM alpha')\n",
    "plt.axis('off')\n",
    "plt.subplot(2,3,6)\n",
    "plt.imshow(est_depth.reshape(image_size),vmin=vmin,vmax=vmax)\n",
    "plt.title('final FM depth')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "#plt.savefig('pose_est.pdf',facecolor=plt.gcf().get_facecolor(), edgecolor='none',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrp2quat(mrp):\n",
    "    mag = mrp @ mrp\n",
    "    q = np.array([1-mag] + list(2*mrp))/(1+mag)\n",
    "    return torch.tensor(q)\n",
    "q1 = mrp2quat(np.array(mrp_true))\n",
    "q2 = mrp2quat(np.array(mrp_final))\n",
    "e1 = torch.acos(torch.clamp((q1 * q2).sum(),-1,1))\n",
    "e2 = torch.acos(torch.clamp((-q1 * q2).sum(),-1,1))\n",
    "rot_err = float((180.0/np.pi)*2*min(e1,e2))\n",
    "\n",
    "R1 = np.array(quaternion_to_matrix(q1))\n",
    "R2 = np.array(quaternion_to_matrix(q2))\n",
    "t_norm = np.linalg.norm(R1.T@np.array(trans_true)-R2.T@np.array(trans_final))\n",
    "trans_err = 100*t_norm/t_model_scale\n",
    "\n",
    "pose_err = np.sqrt(rot_err*trans_err)\n",
    "print('init. pose error of {:04.1f} with rot. of {:04.1f} deg and trans. of {:04.1f}%'.format(init_pose_err,rand_rot,rand_trans))\n",
    "print('final pose error of {:04.1f} with rot. of {:04.1f} deg and trans. of {:04.1f}%'.format(pose_err,rot_err,trans_err))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
