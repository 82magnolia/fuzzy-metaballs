{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qkX7DiM6rmeM"
   },
   "source": [
    "# Shape from Silhouette With Rays\n",
    "Compare to PyTorch3D `Fit a mesh with texture` sample. We only use a silhouette loss (PyTorch sample uses color, silhouette, edge, normal and laplacian loss terms). In our testing, this runs about 1,000x faster than the PyTorch  example on our CPU hardware, although it doesn't reconstruct color. \n",
    "\n",
    "This example differs from the other in that camera poses are implicitly stored in the camera rays data structure and there is no traditional pose optimization possible. However, here we can run simple (no batch) gradient descent on the entire dataset at once, which should be faster for large parallel computers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from util import image_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# io utils\n",
    "from pytorch3d.io import load_objs_as_meshes\n",
    "\n",
    "# 3D transformations functions\n",
    "from pytorch3d.transforms import so3_log_map\n",
    "\n",
    "# rendering components\n",
    "from pytorch3d.renderer import (\n",
    "    FoVPerspectiveCameras, look_at_view_transform, \n",
    "    RasterizationSettings, MeshRenderer, MeshRasterizer,\n",
    "    HardPhongShader, PointLights, TexturesVertex\n",
    ")\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and generate views with PyTorch3D\n",
    "we're using the cow model from Keenan Crane, featured in the PyTorch3D tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch_device = torch.device('cuda')\n",
    "else:\n",
    "    torch_device = torch.device('cpu')\n",
    "#torch_device = torch.device(\"cpu\")\n",
    "mesh = load_objs_as_meshes(['data/cow.obj'], device=torch_device)\n",
    "\n",
    "# seems sane to fetch/estimate scale\n",
    "shape_scale = float(mesh.verts_list()[0].std(0).mean())*3 \n",
    "t_model_scale = np.ptp(np.array(mesh.verts_list()[0]),0).mean()\n",
    "print('model is {:.2f}x the size of the cow'.format(shape_scale/1.18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is simply the dataset generation code, taken from the PyTorch3D tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_views = 20\n",
    "image_size = (64,64)\n",
    "vfov_degrees = 60\n",
    "\n",
    "if False:\n",
    "    # Get a batch of viewing angles like PyT3D\n",
    "    elev = torch.linspace(0, 360, num_views)\n",
    "    azim = torch.linspace(-180, 180, num_views)\n",
    "else:\n",
    "    # Get a batch of views that cover the scene well and are all unique\n",
    "    import scipy.stats.qmc\n",
    "    rand_angles_sample = scipy.stats.qmc.Sobol(2,scramble=False).random(num_views+1)[1:]\n",
    "    rand_angles = (rand_angles_sample*np.array([360.0,360.0]) + np.array([0,-180.0])).astype(np.float32)\n",
    "    elev = rand_angles[:,0]\n",
    "    azim = rand_angles[:,1]\n",
    "\n",
    "lights = PointLights(device=torch_device, location=[[0.0, 0.0, -3.0*shape_scale]])\n",
    "R, T = look_at_view_transform(dist=2.7*shape_scale, elev=elev, azim=azim)\n",
    "cameras = FoVPerspectiveCameras(device=torch_device, R=R, T=T, \n",
    "                                znear=shape_scale, zfar=100*shape_scale, fov=vfov_degrees)\n",
    "camera = FoVPerspectiveCameras(device=torch_device, R=R[None, 1, ...], \n",
    "                                  T=T[None, 1, ...],\n",
    "                                  znear=shape_scale, zfar=100*shape_scale, fov=vfov_degrees) \n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=image_size, \n",
    "    blur_radius=0.0, \n",
    "    faces_per_pixel=1, \n",
    ")\n",
    "\n",
    "renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=camera, \n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=HardPhongShader(\n",
    "        device=torch_device, \n",
    "        cameras=camera,\n",
    "        lights=lights\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create a batch of meshes by repeating the cow mesh and associated textures. \n",
    "# Meshes has a useful `extend` method which allows us do this very easily. \n",
    "# This also extends the textures. \n",
    "meshes = mesh.extend(num_views)\n",
    "\n",
    "# Render the cow mesh from each viewing angle\n",
    "target_images = renderer(meshes, cameras=cameras, lights=lights)\n",
    "\n",
    "# Our multi-view cow dataset will be represented by these 2 lists of tensors,\n",
    "# each of length num_views.\n",
    "target_rgb = [target_images[i, ..., :3] for i in range(num_views)]\n",
    "target_cameras = [FoVPerspectiveCameras(device=torch_device, R=R[None, i, ...], \n",
    "                                           T=T[None, i, ...], znear=shape_scale, zfar=100*shape_scale, fov=vfov_degrees) for i in range(num_views)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_images = target_images.cpu().numpy()\n",
    "target_sil = np_images[:,:,:,3]\n",
    "image_grid(np_images, rows=4, cols=5, rgb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Fuzzy Metaball renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "\n",
    "import jax\n",
    "#jax.config.update('jax_platform_name', 'cpu')\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import fm_render\n",
    "# when using probs, old settings\n",
    "#hyperparams = np.array([-13.69159107,  -2.67968404,   0.71023575,   6.36908448,  -5.42242999])\n",
    "# when using stds, new settings,. beta1 does nothing\n",
    "hyperparams = np.array([-8.96549948, -2.78121285, -0.08753679,  6.41910729, -5.4442808 ])\n",
    "NUM_MIXTURE = 40\n",
    "beta1 = jnp.float32(np.exp(hyperparams[0]))\n",
    "beta2 = jnp.float32(np.exp(hyperparams[1]))\n",
    "beta3 = jnp.float32(np.exp(hyperparams[2]))\n",
    "beta4 = jnp.float32(np.exp(hyperparams[3]))\n",
    "beta5 = jnp.float32(-np.exp(hyperparams[4]))\n",
    "\n",
    "render_jit = jax.jit(fm_render.render_func_rays)\n",
    "obj_scale = 1/(120*shape_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a reasonable initialization to check\n",
    "show_vgmm_init = False\n",
    "if show_vgmm_init:\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    vgmm_model = GaussianMixture(NUM_MIXTURE)\n",
    "\n",
    "    import trimesh\n",
    "    trimesh_mesh = trimesh.Trimesh(mesh.verts_list()[0],mesh.faces_list()[0])\n",
    "    vol_samples = trimesh.sample.volume_mesh(trimesh_mesh,10000)\n",
    "\n",
    "    vgmm_model.fit(vol_samples)\n",
    "\n",
    "    means = jnp.array(vgmm_model.means_)\n",
    "    prec = jnp.array(vgmm_model.precisions_cholesky_)\n",
    "    weights_log = jnp.log(jnp.array(vgmm_model.weights_) + 1e-6)\n",
    "    weights = np.exp(weights_log)\n",
    "    weights /= np.sum(weights)\n",
    "    obj_scale = (weights[:,None] * means).std(0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a Fuzzy Metaballs model from random blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_vgmm_init:\n",
    "    rand_mean = np.random.multivariate_normal(mean=means.mean(0),cov=0.8*jnp.cov(means,rowvar=False),size=NUM_MIXTURE)\n",
    "    rand_weight_log = jnp.log(np.ones(NUM_MIXTURE)/NUM_MIXTURE)\n",
    "    rand_sphere_size = jnp.diag(prec.mean(0)).mean()\n",
    "    rand_prec = jnp.array([np.identity(3)*rand_sphere_size for _ in prec])\n",
    "else:\n",
    "    rand_mean = np.random.multivariate_normal(mean=[0,0,0],cov=1e-1*np.identity(3)*shape_scale,size=NUM_MIXTURE)\n",
    "    rand_weight_log = jnp.log(np.ones(NUM_MIXTURE)/NUM_MIXTURE)\n",
    "    rand_sphere_size = 13\n",
    "    rand_prec = jnp.array([np.identity(3)*rand_sphere_size/shape_scale for _ in range(NUM_MIXTURE)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pyt3dcamera_rays(cam, image_size = image_size):\n",
    "    height, width = image_size\n",
    "    cx = (width-1)/2\n",
    "    cy = (height-1)/2\n",
    "    f = (height/np.tan((np.pi/180)*float(cam.fov[0])/2))*0.5\n",
    "    K = np.array([[f, 0, cx],[0,f,cy],[0,0,1]])\n",
    "    pixel_list = (np.array(np.meshgrid(width-np.arange(width)-1,height-np.arange(height)-1,[0]))[:,:,:,0]).reshape((3,-1)).T\n",
    "\n",
    "    camera_rays = (pixel_list - K[:,2])/np.diag(K)\n",
    "    camera_rays[:,-1] = 1\n",
    "    \n",
    "    translation = np.array(-cam.R[0]@cam.T[0])\n",
    "\n",
    "    camera_rays = camera_rays @ np.array(cam.R[0]).T \n",
    "    trans = np.tile(translation[None],(camera_rays.shape[0],1))\n",
    "    \n",
    "    rays_trans = np.stack([camera_rays,trans],1)\n",
    "    return jnp.array(rays_trans)\n",
    "\n",
    "cameras_list = [convert_pyt3dcamera_rays(cam) for cam in target_cameras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_vgmm_init:\n",
    "    alpha_results = []\n",
    "    for camera_rays in cameras_list:\n",
    "        est_depth, est_probs = render_jit(means,prec,weights_log,camera_rays,beta1/obj_scale,beta2/obj_scale,beta3)\n",
    "        est_alpha = jnp.tanh(beta4*(jnp.exp(est_probs).sum(0)+beta5) )*0.5 + 0.5\n",
    "        alpha_results.append(est_alpha.reshape(image_size))\n",
    "\n",
    "alpha_results_rand = []\n",
    "alpha_results_rand_depth = []\n",
    "for camera_rays in cameras_list:\n",
    "    est_depth, est_probs = render_jit(rand_mean,rand_prec,rand_weight_log,camera_rays,beta1/obj_scale,beta2/obj_scale,beta3)\n",
    "    est_alpha = jnp.tanh(beta4*(jnp.exp(est_probs).sum(0)+beta5) )*0.5 + 0.5\n",
    "    alpha_results_rand.append(est_alpha.reshape(image_size))\n",
    "    est_depth = np.array(est_depth)\n",
    "    est_depth[est_alpha < 0.5] = np.nan\n",
    "    alpha_results_rand_depth.append(est_depth.reshape(image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_grid(target_sil, rows=4, cols=5, rgb=False)\n",
    "plt.gcf().subplots_adjust(top=0.92)\n",
    "plt.suptitle('Reference Masks')\n",
    "if show_vgmm_init:\n",
    "    image_grid(alpha_results, rows=4, cols=5, rgb=False)\n",
    "    plt.gcf().subplots_adjust(top=0.92)\n",
    "    plt.suptitle('vGMM Masks')\n",
    "image_grid(alpha_results_rand, rows=4, cols=5, rgb=False,cmap='Greys')\n",
    "plt.gcf().subplots_adjust(top=0.92)\n",
    "plt.suptitle('random init masks')\n",
    "image_grid(alpha_results_rand_depth, rows=4, cols=5, rgb=False)\n",
    "plt.gcf().subplots_adjust(top=0.92)\n",
    "plt.suptitle('SFS Fuzzy Metaball Initialization')\n",
    "#plt.savefig('sfs_init.pdf',facecolor=plt.gcf().get_facecolor(), edgecolor='none',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize from a random cloud to a shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params,true_alpha):\n",
    "    CLIP_ALPHA = 1e-6\n",
    "    means,prec,weights_log,camera_rays,beta1,beta2,beta3,beta4,beta5 = params\n",
    "    render_res = render_jit(means,prec,weights_log,camera_rays,beta1,beta2,beta3)\n",
    "\n",
    "    est_alpha = jnp.tanh(beta4*(jnp.exp(render_res[1]).sum(0)+beta5) )*0.5 + 0.5\n",
    "    est_alpha = jnp.clip(est_alpha,CLIP_ALPHA,1-CLIP_ALPHA)\n",
    "    mask_loss = - ((true_alpha * jnp.log(est_alpha)) + (1-true_alpha)*jnp.log(1-est_alpha))\n",
    "    return mask_loss.mean()\n",
    "grad_render3 = jax.jit(jax.value_and_grad(objective))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.example_libraries import optimizers\n",
    "\n",
    "# Number of optimization steps\n",
    "Niter = int(round(2000/(len(target_sil))))\n",
    "# number of images to batch gradients over\n",
    "\n",
    "loop = tqdm(range(Niter))\n",
    "\n",
    "# babysit learning rates\n",
    "# adjust_lr = DegradeLR(1e-3,0.5,train_size//2,train_size//10,-1e-3)\n",
    "\n",
    "opt_init, opt_update, opt_params = optimizers.adam(3e-2)\n",
    "tmp = [rand_mean,rand_prec,rand_weight_log]\n",
    "opt_state = opt_init(tmp)\n",
    "\n",
    "all_cameras = jnp.array(cameras_list).reshape((-1,2,3))\n",
    "all_sils = jnp.array(target_sil.ravel()).astype(jnp.float32)\n",
    "\n",
    "losses = []\n",
    "accum_grad = None\n",
    "grad_counter = 0\n",
    "\n",
    "for i in loop:\n",
    "    p = opt_params(opt_state)\n",
    "    val,g = grad_render3([p[0],p[1],p[2],all_cameras,beta1/obj_scale,beta2/obj_scale,beta3,beta4,beta5],all_sils)   \n",
    "    opt_state = opt_update(i, g[:3], opt_state)\n",
    "   \n",
    "    val = float(val)\n",
    "    losses.append(val)\n",
    "    loop.set_description(\"total_loss = %.3f\" % val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mean, final_prec, final_weight_log = opt_params(opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('convergence plot')\n",
    "plt.plot(losses,marker='.',lw=0,ms=5,alpha=0.5)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('log loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_results_final = []\n",
    "alpha_results_depth = []\n",
    "for camera_rays in cameras_list:\n",
    "    est_depth, est_probs = render_jit(final_mean,final_prec,final_weight_log,camera_rays,beta1/obj_scale,beta2/obj_scale,beta3)\n",
    "    est_alpha = jnp.tanh(beta4*(jnp.exp(est_probs).sum(0)+beta5) )*0.5 + 0.5\n",
    "    alpha_results_final.append(est_alpha.reshape(image_size))\n",
    "    \n",
    "    est_depth = np.array(est_depth)\n",
    "    \n",
    "    est_depth[est_alpha < 0.5] = np.nan\n",
    "    alpha_results_depth.append(est_depth.reshape(image_size))\n",
    "image_grid(target_sil, rows=4, cols=5, rgb=False)\n",
    "plt.gcf().subplots_adjust(top=0.92)\n",
    "plt.suptitle('Reference Masks')\n",
    "\n",
    "image_grid(alpha_results_final, rows=4, cols=5, rgb=False)\n",
    "plt.gcf().subplots_adjust(top=0.92)\n",
    "plt.suptitle('Final masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_grid(alpha_results_depth, rows=4, cols=5, rgb=False,vmin=2.*shape_scale,vmax=3.*shape_scale)\n",
    "plt.gcf().subplots_adjust(top=0.92)\n",
    "plt.suptitle('SFS results')\n",
    "plt.tight_layout()\n",
    "#plt.savefig('sfs_res.pdf',facecolor=plt.gcf().get_facecolor(), edgecolor='none',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('fuzzy_cow_shape_rays.pkl','wb') as fp:\n",
    "    pickle.dump([final_mean,final_prec,final_weight_log],fp)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
